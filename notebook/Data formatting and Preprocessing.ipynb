{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e000f5-9e25-4924-a1bb-16820c26cb25",
   "metadata": {},
   "source": [
    "# Preprocess Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "864b529a-66b8-43de-956c-66d1aa86cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer \n",
    "import numpy as np\n",
    "\n",
    "import spacy \n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4df4aec-63fb-4eab-be80-05efce130377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved data \n",
    "def load(filename):\n",
    "    with open(f'./data/{filename}.p', 'rb') as fp:\n",
    "        job_dict = pickle.load(fp)\n",
    "    return job_dict\n",
    "\n",
    "def save(final_file):\n",
    "    with open(f'./data/job_dict.p', 'wb') as fp:\n",
    "        pickle.dump(final_file,file=fp,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_data(file):\n",
    "    with open(file,\"r\",encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_txt(file):\n",
    "    with open(file,\"r\",encoding=\"utf-8\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            line = line.replace(\"\\n\",\"\")\n",
    "            data.append(line)\n",
    "            \n",
    "    return data\n",
    "\n",
    "def save_data(file, data):\n",
    "    \n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4470f69-ac99-432e-9a0c-d9faa9b274c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_roles_links(job_dict):\n",
    "\n",
    "\n",
    "    data_science = []\n",
    "    data_engineer = []\n",
    "    machine_learning = []\n",
    "    analyst = []\n",
    "    art_int = []\n",
    "    # left_over= []\n",
    "    # statistics= []\n",
    "    # job_dict\n",
    "    i = 0\n",
    "    for listing,key in zip(job_dict,list(job_dict.keys())):\n",
    "        i+=1\n",
    "        inf = job_dict.get(key)\n",
    "        role = inf.get('role')\n",
    "        ds = bool(re.search(\".*[Dd]ata\\s.*[Ssc]ien.*\", role))\n",
    "        de = bool(re.search(\".*[Dd]ata\\s.*[Ee]ngine.*\", role))\n",
    "        ml_long = bool(re.search(\".*[Mm]achine\\s.*[Ll]earn.*\",role))\n",
    "        ml = bool(re.search(\".*[Mm][Ll]\\s.*\", role))\n",
    "        anal = bool(re.search(\".*[Dd]ata\\s.*[Aa]nal.*\", role))\n",
    "        ai_long = bool(re.search(\".*[Aa]rtificial\\s.*[Ii]ntelligence.*\", role))\n",
    "        ai = bool(re.search(\".*\\s[Aa][Ii]\\s.*\", role))\n",
    "        # stat = bool(re.search(\".*[Sst]at.*\\s.*[Pp]rogram.*\", role))\n",
    "        if(ds):\n",
    "            data_science.append(listing)\n",
    "        elif(de):\n",
    "            data_engineer.append(listing)\n",
    "        elif(ml_long or ml):\n",
    "            machine_learning.append(listing)\n",
    "        elif(anal):\n",
    "            analyst.append(listing)\n",
    "        elif(ai or ai_long):\n",
    "            art_int.append(listing)\n",
    "        # elif(stat):\n",
    "        #     statistics.append(listing)\n",
    "        # else:\n",
    "        #     left_over.append(listing)\n",
    "    return (analyst)+(machine_learning)+(data_science)+(art_int)+(data_engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f735b84-7404-4619-83e9-4ded0a5bdd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_roles(job_links):\n",
    "    \n",
    "    roles = []\n",
    "    \n",
    "    for link in job_links:\n",
    "        \n",
    "        job = job_dict.get(link)\n",
    "        roles.append(job.get('role'))\n",
    "    \n",
    "    return roles\n",
    "\n",
    "def get_descriptions(job_links):\n",
    "    \n",
    "    descriptions = []\n",
    "    \n",
    "    for link in job_links:\n",
    "        \n",
    "        job = job_dict.get(link)\n",
    "        descriptions.append(str(job.get('description')).replace('\\n',\" \"))\n",
    "        # print(descriptions)\n",
    "        # a\n",
    "        # # break\n",
    "        \n",
    "    return descriptions\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    clean_des = []\n",
    "    for description in descriptions:\n",
    "        if(description=='None'):\n",
    "            continue\n",
    "        else:\n",
    "            clean_des.append(description)\n",
    "    return clean_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d436ed0-1895-43d0-8d2d-58f5a7089790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load main data\n",
    "dict_1 = load_data('data/job_street.json')\n",
    "dict_2 = load_data('data/linkedin.json')\n",
    "job_dict_malaysia  = {}\n",
    "job_dict_malaysia.update(dict_1)\n",
    "job_dict_malaysia.update(dict_2)\n",
    "\n",
    "# load test data\n",
    "# job_dict= load_data(\"data/test_job_dict_linkedin.json\")\n",
    "# job_links = get_all_roles_links(job_dict)\n",
    "# roles = get_roles(job_links)\n",
    "# role_descriptions = get_descriptions(job_links)\n",
    "# role_descriptions = clean_descriptions(role_descriptions)\n",
    "# save_data(\"data/test_description.txt\",role_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6165f541-e004-4d07-b8de-b4d57e2e342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"data/job_dict_malaysia.json\",job_dict_malaysia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3575e9ec-a1ac-4c6a-acc4-8fe9be93e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def generate_better_data(file):\n",
    "    \n",
    "    data = load_txt(file)\n",
    "    new_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        if(len(data)>2):\n",
    "            new_data.append(item.lower())\n",
    "    \n",
    "    return data+new_data\n",
    "\n",
    "def create_patterns(file,type,patterns):\n",
    "    \n",
    "    data = generate_better_data(file)\n",
    "    # patterns = []\n",
    "    for item in data:\n",
    "        pattern = {\n",
    "                    \"label\": type,\n",
    "                    \"pattern\": item\n",
    "        }\n",
    "        patterns.append(pattern)\n",
    "    return (patterns)\n",
    "\n",
    "# @Language.component\n",
    "def generate_rules(patterns):\n",
    "    nlp = English()\n",
    "    ruler = EntityRuler(nlp)\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    nlp.to_disk(\"ner_ruler\")\n",
    "    \n",
    "def test_model(model,text):\n",
    "    doc = nlp(text)\n",
    "    results = []\n",
    "    for ent in doc.ents:\n",
    "        results.append(ent.text)\n",
    "    return (results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc984d-2844-477c-870b-dbba86377ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('data/description.txt',role_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f30af7-efa0-410d-bde4-bbd028694325",
   "metadata": {},
   "source": [
    "# Generate Entity Rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09fee956-9ece-4f20-bc5d-2de3f4659d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "patterns1 = create_patterns(\"./data/programming_languages.txt\",\"PROGLANG\",patterns)\n",
    "patterns2 = create_patterns(\"./data/dt_tools.txt\",\"TOOL\",patterns1)\n",
    "patterns3 = create_patterns(\"./data/libraries&packages.txt\",\"LIBRARY\",patterns2)\n",
    "patterns4 = create_patterns(\"./data/education.txt\",\"EDUCATION\",patterns3)\n",
    "generate_rules(patterns4)\n",
    "nlp = spacy.load(\"ner_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a70c1a94-655f-48fb-82f1-858e35822abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data(\"./data/test_description.txt\")[2]\n",
    "r = test_model(nlp,text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
